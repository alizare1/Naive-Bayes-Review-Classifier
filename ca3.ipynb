{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit245f2f1847444152bf72055fb224c996",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CA3\n",
    "## Mohammad Ali Zare\n",
    "### 810197626\n",
    "\n",
    "In this assignment we must infer if a Digikala review is positive or negative using naive bayes. The iniital information is gathered from a train dataset and then we use it on a test dataset.\n",
    "\n",
    "Naive bayes uses Bayes rule and conditional independence as its base. It's called naive because of its assumption of independence of the features. For example in a spam filter model, it doesn't care about order of the words in a message or sentence and assumes them independent (aka bag of words). This model uses bag of words similar to spam filter.\n",
    "\n",
    "Naive bayes, although simple, still can get an acceptable result therefore it's used mostly for a fast, easy and experimental model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import hazm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "source": [
    "------------\n",
    "### Loaded Data\n",
    "\n",
    "The comment_train.csv file contains our train data and comment_test.csv contains the test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('comment_train.csv')\n",
    "test_data = pd.read_csv('comment_test.csv')"
   ]
  },
  {
   "source": [
    "---------\n",
    "### Stop Words\n",
    "\n",
    "Conjuctions, punctuation marks and whitespaces are chosen as stop words. Hazm's stop word list was also tried and the results didn't change much.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = ['و', 'یا', 'را', '!', '؟', '?', '.', ',', '،', '\\r', '\\n', '\\t', 'به', 'از', 'ُ', 'ً', 'ٍ']\n",
    "# stop_list = hazm.stopwords_list()"
   ]
  },
  {
   "source": [
    "--------\n",
    "### Title and Comment\n",
    "\n",
    "The title of reviews were appended to the comment with a weight of 2. Meaning they were repated twice in the comment so they can have higher effect on the result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment'] = train_data['comment'] + ' ' + 2*(train_data['title']+ ' ')\n",
    "test_data['comment'] = test_data['comment'] + ' ' + 2*(test_data['title']+ ' ')"
   ]
  },
  {
   "source": [
    "------------\n",
    "## Pre-Process\n",
    "\n",
    "Diffrent approaches were tried for the pre-process including removing stop words and Hazm's stemmer, lemmatizer and normalizer. Hazm's stemmer didn't function very well as it removed ending letters of most of the words that it shouldn't have. The final pre-process is removing stop words, then normalizing each word and then lemmatizing that word.\n",
    "\n",
    "### Lemmatization vs Stemming\n",
    "\n",
    "Both lemmatization and stemming try to find the root of a word so different variation of words and verbs become same words. This way we can get more accurate histogram and frequency of those words.\n",
    "\n",
    "Stemming mostly relies on cutting of the suffixes and prefixes of a word to return its root, for example changing **می‌رفتم** to **رفت**. But sometimes it fails and cuts some parts that are in the root, Eg. changing **پایان** to **پا**. An english example would be **clearly** to **clear**.\n",
    "\n",
    "On the other hand, lemmatization tries to find root of the word in the context and takes its meaning into account, it may use a database of words and their meanings to do its job. As an example it can change **می‌روم** to **رفت** but stemming may fail and change it to **می‌رو**. Or an english example is changing **worse** to **bad**.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = hazm.Stemmer()\n",
    "lem = hazm.Lemmatizer()\n",
    "normalizer = hazm.Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_words(x):\n",
    "    return x not in stop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_n_norm(x):\n",
    "    splitted = lem.lemmatize(normalizer.normalize(x)).split('#')\n",
    "    return splitted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(words):\n",
    "    result = list(map(lem_n_norm, filter(filter_stop_words, words)))\n",
    "    return list(filter(None, result))"
   ]
  },
  {
   "source": [
    "----------\n",
    "## Naive Bayes Classifier\n",
    "\n",
    "As explained in the introduction it uses Bayes rule and assumption of conditional independence as its base.\n",
    "\n",
    "### Model\n",
    "\n",
    "This model has two classes (**recommended** and **not_recommended**). Features of the classes are the words used in each class, ie. the words appearing in each review of the class so if a word is repeated many times in recommended reviews, a comment with that word gets higher chance of being classified as recommended.\n",
    "\n",
    "#### Bag of Words\n",
    "\n",
    "The bag of words model is used so we treat all the words the same regardless of their position in a sentence. To make the bag of words, we combine all the recommended reviews and tokenize them, then put them in a single list of words. We do the same for not_recommended reviews. We also treat probability of a word independent of another word given its class.\n",
    "\n",
    "### Process\n",
    "\n",
    "First we count occurance of each word in the words list for recommended and not_recommended.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqs(rec_list, not_rec_list):\n",
    "    rec_freq = dict(collections.Counter(rec_list))\n",
    "    not_rec_freq = dict(collections.Counter(not_rec_list))\n",
    "    return rec_freq, not_rec_freq, len(rec_list), len(not_rec_list)\n"
   ]
  },
  {
   "source": [
    "#### Prior Probability\n",
    "\n",
    "The initial probability of a message being recommended or not_recommended without having any evidences. It's calculated by dividing number of each class by the total reviews:\n",
    "\n",
    "$P(recommended) = \\dfrac{recommended\\_review\\_count}{total\\_review\\_count}$\n",
    "\n",
    "It's 0.5 for both classes:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "(train_data['recommend'] == 'recommended').sum() / len(train_data)"
   ]
  },
  {
   "source": [
    "#### Likelihood\n",
    "The probability of appearance of a word in a review given it is labaled recommended or not_recommended. It would be (it's easy to calculate):\n",
    "\n",
    "$P(word\\ |\\ recommended) = \\dfrac{frequency\\_in\\_recommended\\_words(word)}{total\\ recommended\\ words\\ count}$\n",
    "\n",
    "#### Evidence\n",
    "\n",
    "Appearance of each word in a review is an evidence\n",
    "\n",
    "#### Posterior\n",
    "\n",
    "Finding probability of being recommended given seeing a word. It isn't easy in a direct way but using the Bayes rule we can calculate it with the other informations:\n",
    "\n",
    "$ P(recommended\\ |\\ word) = \\dfrac{P(recommended)*P(word\\ |\\ recommended)}{P(word)} $\n",
    "\n",
    "P(word) is the evidence here.\n",
    "\n",
    "### Labeling in this problem\n",
    "\n",
    "We can use the said equations to get posterior probablities of each review for beaing recommended or not_recommended and by comparing them, we label that review based on which of them has a higher value. Note that we don't need to calculate the evidence as it is equal in both classes. \n",
    "\n",
    "We assumed each word conditionally independent given recommended/not_recommended so we just multiply the probabilities.\n",
    "\n",
    "As an example for the sentance **word1 word2**:\n",
    "\n",
    "$P(recommended\\ |\\ sentance) = P(recommended)*P(word1 | recommended)*P(word2 | recommended)$\n",
    "\n",
    "We calculate this for not_recommended too and then compare them.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "-------\n",
    "### Additive Smoothing\n",
    "\n",
    "Sometimes a word can appear in the not_recommended reviews training data but not in the recommended. For this reason the posterior probability of not_recommended would be equal to zero. For example it we didn't see **word2** in recommended reviews of training data this would happen:\n",
    "\n",
    "$P(word2\\ |\\ recommended) = 0$\n",
    "\n",
    "$P(recommended\\ |\\ sentance) = P(recommended)*P(word1 | recommended)*P(word2 | recommended) = 0$\n",
    "\n",
    "So the review would be classified as not_recommended, no matter what the other words are. \n",
    "\n",
    "To solve this problem we use Additive Smoothing to eliminate these 0 probabilities.\n",
    "\n",
    "We add an Alpha to the count of each word in all classes, and assign alpha to count of those missing words in each class. So in the example, **word2** count would be Alpha for recommended words list, and Alpha + prev_count for the not_recommended words lits.\n",
    "\n",
    "Alpha = 1 was used for this problem.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoothed_freqs(rec_list, not_rec_list):\n",
    "    rec_freq = dict(collections.Counter(rec_list))\n",
    "    not_rec_freq = dict(collections.Counter(not_rec_list))\n",
    "    rec_word_count = len(rec_list)\n",
    "    not_rec_word_count = len(not_rec_list)\n",
    "\n",
    "    for word in rec_list:\n",
    "        rec_freq[word] += 1\n",
    "        if word not in not_rec_freq:\n",
    "            not_rec_word_count += 1\n",
    "            not_rec_freq[word] = 1\n",
    "\n",
    "    for word in not_rec_list:\n",
    "        not_rec_freq[word] += 1\n",
    "        if word not in rec_freq:\n",
    "            rec_word_count += 1\n",
    "            rec_freq[word] = 1\n",
    "    \n",
    "    return rec_freq, not_rec_freq, rec_word_count, not_rec_word_count"
   ]
  },
  {
   "source": [
    "--------------\n",
    "--------------\n",
    "### Label function\n",
    "\n",
    "Very small numbers were considered 0 by Python so the Log function was used for probabilites so we can sum them instead of multiplying them and avoid getting very small values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(comment, info, pre_proc=None):\n",
    "    words = hazm.word_tokenize(comment)\n",
    "\n",
    "    if pre_proc:\n",
    "        words = pre_proc(words)\n",
    "\n",
    "    rec_freq, not_rec_freq, rec_word_count, not_rec_word_count = info\n",
    "\n",
    "    rec_score = math.log(0.5) # prior\n",
    "    not_rec_score = math.log(0.5)\n",
    "\n",
    "    for word in words:\n",
    "        if word not in rec_freq and word not in not_rec_freq: #ignore extra words\n",
    "            continue\n",
    "        \n",
    "        if word not in rec_freq: # no smoothing\n",
    "            rec_score = float('-inf')\n",
    "            break\n",
    "\n",
    "        if word not in not_rec_freq: # no smoothing\n",
    "            not_rec_score = float('-inf')\n",
    "            break\n",
    "\n",
    "\n",
    "        rec_score += math.log(rec_freq[word] / rec_word_count)\n",
    "        not_rec_score += math.log(not_rec_freq[word] / not_rec_word_count)\n",
    "    \n",
    "    \n",
    "    if rec_score > not_rec_score:\n",
    "        return 'recommended'\n",
    "    else:\n",
    "        return 'not_recommended'"
   ]
  },
  {
   "source": [
    "--------\n",
    "### Creating Bag of Words\n",
    "\n",
    "All the comments are added to a single list for each class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_words = []\n",
    "not_rec_words = []\n",
    "for i, row in train_data.iterrows():\n",
    "    if row['recommend'] == 'recommended':\n",
    "        rec_words += hazm.word_tokenize(row['comment'])\n",
    "    else:\n",
    "        not_rec_words += hazm.word_tokenize(row['comment'])"
   ]
  },
  {
   "source": [
    "--------------\n",
    "\n",
    "Here the label function is called with and without pre-processing and smoothing:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nothing_info = get_freqs(rec_words, not_rec_words)\n",
    "smoothed_info = get_smoothed_freqs(rec_words, not_rec_words)\n",
    "pre_info = get_freqs(pre_process(rec_words), pre_process(not_rec_words))\n",
    "pre_smoothed_info = get_smoothed_freqs(pre_process(rec_words), pre_process(not_rec_words))\n",
    "\n",
    "test_data['pre_smooth'] = test_data['comment'].apply(label, args=[pre_smoothed_info, pre_process])\n",
    "test_data['smooth'] = test_data['comment'].apply(label, args=[smoothed_info])\n",
    "test_data['pre'] = test_data['comment'].apply(label, args=[pre_info, pre_process])\n",
    "test_data['nothing'] = test_data['comment'].apply(label, args=[nothing_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(label):\n",
    "    correct_recs_detected = ((test_data['recommend'] == test_data[label]) & (test_data[label] == 'recommended')).sum()\n",
    "    all_recs_detected = (test_data[label] == 'recommended').sum()\n",
    "    total_recs = (test_data['recommend'] == 'recommended').sum()\n",
    "\n",
    "    accuracy = (test_data['recommend'] == test_data[label]).sum() / len(test_data) * 100\n",
    "    precision = correct_recs_detected / all_recs_detected * 100\n",
    "    recall = correct_recs_detected / total_recs * 100\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print('-------------------------')\n",
    "    print(label + ':\\n')\n",
    "    print(f'{\"Accuracy\":>10}: \\t {accuracy :.2f} %\\n')\n",
    "    print(f'{\"Precision\":>10}: \\t {precision :.2f} %\\n')\n",
    "    print(f'{\"Recall\":>10}: \\t {recall :.2f} %\\n')\n",
    "    print(f'{\"F1\":>10}: \\t {f1 :.2f} %\\n')\n",
    "\n"
   ]
  },
  {
   "source": [
    "-----------\n",
    "### Evaluation\n",
    "\n",
    "## Precision\n",
    "\n",
    "If we only use precision, in a case if we detect only on recommended and that is correct, we get 100%, so it can't be used alone. Generally if our model detects a few comments as recommended it can get a high precision although the model is not very good.\n",
    "\n",
    "## Recall\n",
    "\n",
    "If we detect lots of recommended comments including many correct and many wrongs ones, we get a high recall value but still the model is not good. For example if we label all the reviews as recommended, it gets 100% recall.\n",
    "\n",
    "## F1\n",
    "\n",
    "To combat the downsides of recall and precision, we get an average of these two values to generate F1 score. F1 is **harmonic mean** of recall and precision. Harmonic mean takes multiple parameters (in this context both recall and precision) into account. This value is a better representation of the correctness of our model.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------------\npre_smooth:\n\n  Accuracy: \t 93.38 %\n\n Precision: \t 92.63 %\n\n    Recall: \t 94.25 %\n\n        F1: \t 93.43 %\n\n"
     ]
    }
   ],
   "source": [
    "print_results('pre_smooth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------------\nsmooth:\n\n  Accuracy: \t 94.88 %\n\n Precision: \t 94.32 %\n\n    Recall: \t 95.50 %\n\n        F1: \t 94.91 %\n\n"
     ]
    }
   ],
   "source": [
    "print_results('smooth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------------\npre:\n\n  Accuracy: \t 90.25 %\n\n Precision: \t 90.25 %\n\n    Recall: \t 90.25 %\n\n        F1: \t 90.25 %\n\n"
     ]
    }
   ],
   "source": [
    "print_results('pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------------\nnothing:\n\n  Accuracy: \t 90.00 %\n\n Precision: \t 89.60 %\n\n    Recall: \t 90.50 %\n\n        F1: \t 90.05 %\n\n"
     ]
    }
   ],
   "source": [
    "print_results('nothing')"
   ]
  },
  {
   "source": [
    "### Results\n",
    "\n",
    "We can see when we use additive smoothing, it improves our model with a noticeable difference. The reason is it prevents our model from deciding a label solely based on a word that wasn't in a class training data and lets the model take more words into account.\n",
    "\n",
    "But our pre-process is not very effective, the reason can be because it takes some context away, for example negative and positive verbs become the same word or some words lose their meanings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "--------\n",
    "### When our model makes mistake\n",
    "\n",
    "In the example below both unique words in the comments has higher score given it's recommended although it's not_recommended. So our model labels it as recommended.\n",
    "\n",
    "The reason can be the context that words are used in. Our Naive Bayes model ignores the context and the sentence completely. But in reality a positive word can have a negative meaning given context and the verb used. For example **ایراد** can be used as **ایراد ندارد** and **ایراد دارد**, these two sentences have different meaning but our model treat the word **ایراد** the same. Also the negative and positive verbs become same verbs in the pre-processing.\n",
    "\n",
    "Another reason can be small stop words set. We haven't considered all neutral words so they give different weights to the classes although they are not really trustable for labeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_freq, not_rec_freq, rec_word_count, not_rec_word_count = pre_smoothed_info\n",
    "wrongs = test_data[test_data['recommend'] != test_data['pre_smooth']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "real label:  not_recommended\nour label:  recommended\ncomment:\n ایراد دستگاه ایراد دستگاه ایراد دستگاه \n\nدستگاه score in recommended words      0.005051801007152709\nدستگاه score in not_recommended words  0.0030817436815646303\nایراد score in recommended words      0.0010745100554896238\nایراد score in not_recommended words  0.0004992769093037669\n"
     ]
    }
   ],
   "source": [
    "print('real label: ', wrongs.iloc[5]['recommend'])\n",
    "print('our label: ', wrongs.iloc[5]['pre_smooth'])\n",
    "print('comment:\\n', wrongs.iloc[5]['comment'])\n",
    "print('')\n",
    "print('دستگاه score in recommended words     ', rec_freq['دستگاه'] / rec_word_count)\n",
    "print('دستگاه score in not_recommended words ', not_rec_freq['دستگاه'] / not_rec_word_count)\n",
    "print('ایراد score in recommended words     ', rec_freq['ایراد'] / rec_word_count)\n",
    "print('ایراد score in not_recommended words ', not_rec_freq['ایراد'] / not_rec_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "real label:  not_recommended\nour label:  recommended\ncomment:\n باسلام خدمت دوستان  من تعجب میکنم از چیه این تعریف میکنن\nگوشی من سامسونگ اس ۶ هستش ۲۵۵۰ \nحالا ۳.۵ بار شارژ میکنه کنار \nبحثم اینجاست \n۲ساعت نیم میکشه شارژ کامل که واقعا خوب نیست فاجعه هستش \nو مورد دیگه اداپتور من فست هستش تازه با فست قشنگ ۷الی۸ ساعت میکشه شارژ بشه \nچیه این خوبه اخه تعریف میکنید \nنه شکل ظاهر مناسب  نه ابعاد خوب \nدیر شارژ میشه \nشارژ کند انجام میده \nتنها مزیت این گارانتی هستش \nتموم شد رفت پاور بانک پاور بانک \n\n------------------\n\nreal label:  recommended\nour label:  not_recommended\ncomment:\n من دوسه ماهی هست این کفشدازردیجی گرفتم متاسفانه کیفیت چسب کفی خوب نیست و از جلو بلند شده و اینکه بنداش کیفیت لازم رو نداره و پا داخلش بو میگیره قالباشم دقیق نیست بنظرم ارزش این پول نداره ... پیشنهاد نمیدم پیشنهاد نمیدم \n\n------------------\n\nreal label:  not_recommended\nour label:  recommended\ncomment:\n این آچار لوله گیر خیلی سنگینه،برای کارمداوم وکسانی که دست وبازوی ضعیفی دارند اصلا مناسب نیست.اگرقبل ازخریدبه دست می گرفتم،ازخریدمنصرف می شدم.. اندرمعایب آچارلوله گیر14اینچ ایران پتک اندرمعایب آچارلوله گیر14اینچ ایران پتک \n\n------------------\n\nreal label:  recommended\nour label:  not_recommended\ncomment:\n این کالا در طول یکروز بدستم رسید که جای تشکر داره \nمنتهی کالا با عکس دیجی کالا تطابق نداره یه مدل دیگه هست ولی کاربردش یکیه . در کل خوبه ارزش خرید داره . شکل ظاهری شکل ظاهری \n\n------------------\n\nreal label:  not_recommended\nour label:  recommended\ncomment:\n این نوعش به درد نمیخوره نوعش که گیاهیه خوبه.من استفاده کردم حس خوبی بهش نداشتم نسبت به نوع دیگه این برند. کلنیل کلنیل \n\n------------------\n\n"
     ]
    }
   ],
   "source": [
    "for i, row in wrongs.tail().iterrows(): \n",
    "    print('real label: ', row['recommend'])\n",
    "    print('our label: ', row['pre_smooth'])\n",
    "    print('comment:\\n',row['comment'])\n",
    "    print('\\n------------------\\n')"
   ]
  }
 ]
}